{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Effects of preprocessing\n",
    "\n",
    "## Preprocessing for LID\n",
    "In general, multiple different preprocessing steps are viable for the LID task. The survey [1] e.g. mentions the following preprocessing steps:\n",
    "\n",
    "- **Case folding**: Convert all characters to lowercase.\n",
    "- **Range compression**: Groups a range of characters into a single logical set to reduce sparcity which is especially useful for languages with large character sets like Chinese.\n",
    "- **Noise removal:** Remove digits, punctuation, special characters, language-independent characters (like URLs, emails, etc.). This is mostly done using heuristics.\n",
    "\n",
    "Other common NLP preprocessing steps, however, might not be suited for the task. They mostly include normalization techniques:\n",
    "\n",
    "- **Removing stop words and diacritics**: As [2] points out, stop words and diacritics are language-specific and useful for the LID task.\n",
    "- **Lemmatization**: Relies on understanding a word's base form, which depends on grammar, morphology, and irregular forms.\n",
    "- **Stemming**: Applies heuristic rules to chop off word endings, but these rules are language-dependent.\n",
    "\n",
    "Language-agnostic approaches to these normalization techniques often rely on rule-based heuristics and are often impractical for a large number of languages. Apart from these methods, one might use statistical, embedding-based or neural methods to learn word structures across languages. However, this would leave the realm of preprocessing for classical ML methods and enter the domain of deep learning.\n",
    "\n",
    "## Khan's WiLi-2018 subset\n",
    "As data exploration already showed, the Khan's WiLi-2018 dataset is already preprocessed. The text is already lowercased and some noise removal has been applied. As the dataset's name suggests, it is already optimized for the LID task. Therefore, we expect no significant performance improvements from further preprocessing. Nevertheless, we at least validate that the lowercase assumption holds true for the entire dataset. Noise removal is not easily validated because it is not obvious what kind of noise removal was applied."
   ],
   "id": "1ce5a64f3a22b8e2"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-20T07:56:42.123939Z",
     "start_time": "2025-02-20T07:56:31.954313Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "\n",
    "from langlens.data import _clean_data\n",
    "\n",
    "# Load the dataset\n",
    "df = pd.read_csv(\"../data/wili_subset_khan.csv\")\n",
    "df = _clean_data(df)"
   ],
   "id": "b4f1c5b5c2d3df49",
   "outputs": [],
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-02-20T07:56:42.185902Z",
     "start_time": "2025-02-20T07:56:42.133319Z"
    }
   },
   "source": [
    "def is_lowercased(text):\n",
    "    return text == text.lower()\n",
    "\n",
    "\n",
    "is_all_lowercased = df['text'].apply(is_lowercased).all()\n",
    "is_all_lowercased"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.True_"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "The dataset has been properly lowercased. Let us evaluate its performance using a simple Naive Bayes classifier on character uni-grams.",
   "id": "96c0bcf4b9e7bc33"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-20T07:56:51.367568Z",
     "start_time": "2025-02-20T07:56:43.062198Z"
    }
   },
   "cell_type": "code",
   "source": "! python ../langlens/main.py baseline --dataset-path ../data/wili_subset_khan.csv --n-gram-type char --vocab-size 512",
   "id": "7eff917119227f7a",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-02-20 08:56:46,291 - langlens.__main__ - INFO - Loading dataset from ../data/wili_subset_khan.csv\n",
      "2025-02-20 08:56:46,445 - langlens.__main__ - INFO - Training dataset size: 17599, Val/Test dataset size: 2200\n",
      "2025-02-20 08:56:46,445 - langlens.__main__ - INFO - Vectorizing and normalizing data...\n",
      "2025-02-20 08:56:49,933 - langlens.langlens.baseline.vectorize - INFO - The vocabulary covers 96.04% of the training data.\n",
      "2025-02-20 08:56:49,984 - langlens.__main__ - INFO - Training classifier...\n",
      "2025-02-20 08:56:50,038 - langlens.__main__ - INFO - Validation set performance:\n",
      "2025-02-20 08:56:50,052 - langlens.langlens.evaluation - INFO -               precision    recall  f1-score   support\n",
      "\n",
      "      Arabic       1.00      0.99      0.99        97\n",
      "     Chinese       1.00      0.97      0.99       102\n",
      "       Dutch       0.98      0.89      0.93        93\n",
      "     English       0.63      0.99      0.77        96\n",
      "    Estonian       0.98      0.95      0.96       101\n",
      "      French       0.95      0.93      0.94        97\n",
      "       Hindi       1.00      0.99      0.99        94\n",
      "  Indonesian       0.97      0.97      0.97        90\n",
      "    Japanese       1.00      0.98      0.99       107\n",
      "      Korean       1.00      0.99      1.00       105\n",
      "       Latin       0.95      0.89      0.92       102\n",
      "     Persian       0.97      1.00      0.99       100\n",
      "  Portuguese       0.99      0.91      0.95       105\n",
      "      Pushto       1.00      0.94      0.97       108\n",
      "    Romanian       1.00      0.98      0.99       100\n",
      "     Russian       0.99      1.00      0.99        95\n",
      "     Spanish       0.99      0.89      0.94       107\n",
      "     Swedish       1.00      0.99      0.99        88\n",
      "       Tamil       1.00      0.99      1.00       112\n",
      "        Thai       1.00      0.99      0.99        96\n",
      "     Turkish       1.00      0.99      1.00       106\n",
      "        Urdu       0.99      0.99      0.99        99\n",
      "\n",
      "    accuracy                           0.96      2200\n",
      "   macro avg       0.97      0.96      0.97      2200\n",
      "weighted avg       0.97      0.96      0.97      2200\n",
      "\n",
      "Figure(1000x800)\n",
      "2025-02-20 08:56:50,754 - langlens.langlens.evaluation - INFO - Variance explained by PCA: [0.31440923 0.14224146]\n",
      "Figure(800x600)\n",
      "2025-02-20 08:56:50,783 - langlens.__main__ - INFO - Test set performance:\n",
      "2025-02-20 08:56:50,795 - langlens.langlens.evaluation - INFO -               precision    recall  f1-score   support\n",
      "\n",
      "      Arabic       1.00      0.99      0.99        94\n",
      "     Chinese       1.00      0.94      0.97        86\n",
      "       Dutch       0.98      0.90      0.94       127\n",
      "     English       0.58      0.99      0.73        90\n",
      "    Estonian       0.99      0.94      0.96        96\n",
      "      French       0.94      0.93      0.94        89\n",
      "       Hindi       1.00      0.99      1.00       110\n",
      "  Indonesian       1.00      0.91      0.95       108\n",
      "    Japanese       1.00      0.99      0.99        81\n",
      "      Korean       1.00      0.99      0.99        98\n",
      "       Latin       0.96      0.87      0.91       101\n",
      "     Persian       0.96      1.00      0.98        99\n",
      "  Portuguese       0.97      0.94      0.95        97\n",
      "      Pushto       0.99      0.92      0.95        91\n",
      "    Romanian       1.00      0.95      0.97       102\n",
      "     Russian       0.98      1.00      0.99       110\n",
      "     Spanish       0.97      0.94      0.96       106\n",
      "     Swedish       0.99      1.00      0.99        99\n",
      "       Tamil       1.00      0.98      0.99       109\n",
      "        Thai       1.00      0.97      0.99       108\n",
      "     Turkish       0.99      0.98      0.98        87\n",
      "        Urdu       1.00      0.98      0.99       112\n",
      "\n",
      "    accuracy                           0.96      2200\n",
      "   macro avg       0.97      0.96      0.96      2200\n",
      "weighted avg       0.97      0.96      0.96      2200\n",
      "\n",
      "Figure(1000x800)\n",
      "2025-02-20 08:56:51,083 - langlens.langlens.evaluation - INFO - Variance explained by PCA: [0.31440923 0.14224146]\n",
      "Figure(800x600)\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "We observe a validation set accuracy of 0.96 and a F1 score of 0.97. This is a good result for a simple baseline model.\n",
    "\n",
    "## Introducing more preprocessing\n",
    "Let us try to improve by removing more \"noise\" from the dataset."
   ],
   "id": "8182c6a076e8e20b"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-20T07:58:59.653529Z",
     "start_time": "2025-02-20T07:58:58.753224Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import re\n",
    "\n",
    "\n",
    "def remove_noise(text):\n",
    "    # Remove URLs\n",
    "    text = re.sub(r'http\\S+|www\\.\\S+', '', text)\n",
    "    # Remove emails\n",
    "    text = re.sub(r'\\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\.[A-Za-z]{2,}\\b', '', text)\n",
    "    # Remove digits\n",
    "    text = re.sub(r'\\d+', '', text)\n",
    "    # Remove punctuation and special characters (excluding spaces)\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)\n",
    "    # Remove extra spaces\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "\n",
    "    return text\n",
    "\n",
    "\n",
    "# Apply preprocessing to the 'text' column\n",
    "df_noise_free = df.copy()\n",
    "df_noise_free['text'] = df_noise_free['text'].apply(remove_noise)\n",
    "\n",
    "# Save the cleaned dataset\n",
    "df_noise_free.to_csv(\"../data/wili_subset_khan_cleaned.csv\", index=False)"
   ],
   "id": "a774a13738e71bac",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "The dataset has been further cleaned. Let us evaluate its performance using a simple Naive Bayes classifier on character uni-grams.",
   "id": "8d451bc8b5035e70"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-20T07:59:27.003298Z",
     "start_time": "2025-02-20T07:59:20.239392Z"
    }
   },
   "cell_type": "code",
   "source": "! python ../langlens/main.py baseline --dataset-path ../data/wili_subset_khan_cleaned.csv --n-gram-type char --vocab-size 512",
   "id": "2ac511b7559dfe80",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-02-20 08:59:22,152 - langlens.__main__ - INFO - Loading dataset from ../data/wili_subset_khan_cleaned.csv\n",
      "2025-02-20 08:59:22,305 - langlens.__main__ - INFO - Training dataset size: 17599, Val/Test dataset size: 2200\n",
      "2025-02-20 08:59:22,305 - langlens.__main__ - INFO - Vectorizing and normalizing data...\n",
      "2025-02-20 08:59:25,549 - langlens.langlens.baseline.vectorize - INFO - The vocabulary covers 96.36% of the training data.\n",
      "2025-02-20 08:59:25,604 - langlens.__main__ - INFO - Training classifier...\n",
      "2025-02-20 08:59:25,649 - langlens.__main__ - INFO - Validation set performance:\n",
      "2025-02-20 08:59:25,662 - langlens.langlens.evaluation - INFO -               precision    recall  f1-score   support\n",
      "\n",
      "      Arabic       0.88      0.99      0.93        97\n",
      "     Chinese       1.00      0.96      0.98       102\n",
      "       Dutch       0.98      0.88      0.93        93\n",
      "     English       0.63      0.99      0.77        96\n",
      "    Estonian       0.97      0.95      0.96       101\n",
      "      French       0.96      0.93      0.94        97\n",
      "       Hindi       1.00      0.99      0.99        94\n",
      "  Indonesian       0.97      0.96      0.96        90\n",
      "    Japanese       1.00      0.98      0.99       107\n",
      "      Korean       1.00      0.99      1.00       105\n",
      "       Latin       0.95      0.89      0.92       102\n",
      "     Persian       0.97      1.00      0.99       100\n",
      "  Portuguese       1.00      0.90      0.95       105\n",
      "      Pushto       0.99      0.94      0.97       108\n",
      "    Romanian       1.00      0.98      0.99       100\n",
      "     Russian       0.99      1.00      0.99        95\n",
      "     Spanish       0.97      0.91      0.94       107\n",
      "     Swedish       1.00      0.99      0.99        88\n",
      "       Tamil       1.00      0.99      1.00       112\n",
      "        Thai       1.00      0.99      0.99        96\n",
      "     Turkish       1.00      0.99      1.00       106\n",
      "        Urdu       1.00      0.86      0.92        99\n",
      "\n",
      "    accuracy                           0.96      2200\n",
      "   macro avg       0.97      0.96      0.96      2200\n",
      "weighted avg       0.97      0.96      0.96      2200\n",
      "\n",
      "Figure(1000x800)\n",
      "2025-02-20 08:59:26,375 - langlens.langlens.evaluation - INFO - Variance explained by PCA: [0.3236703  0.13664327]\n",
      "Figure(800x600)\n",
      "2025-02-20 08:59:26,405 - langlens.__main__ - INFO - Test set performance:\n",
      "2025-02-20 08:59:26,415 - langlens.langlens.evaluation - INFO -               precision    recall  f1-score   support\n",
      "\n",
      "      Arabic       0.91      0.99      0.95        94\n",
      "     Chinese       1.00      0.94      0.97        86\n",
      "       Dutch       0.97      0.89      0.93       127\n",
      "     English       0.58      0.98      0.73        90\n",
      "    Estonian       0.99      0.92      0.95        96\n",
      "      French       0.93      0.93      0.93        89\n",
      "       Hindi       1.00      0.99      1.00       110\n",
      "  Indonesian       0.99      0.91      0.95       108\n",
      "    Japanese       1.00      0.99      0.99        81\n",
      "      Korean       1.00      0.99      0.99        98\n",
      "       Latin       0.96      0.87      0.91       101\n",
      "     Persian       0.96      1.00      0.98        99\n",
      "  Portuguese       1.00      0.94      0.97        97\n",
      "      Pushto       0.99      0.92      0.95        91\n",
      "    Romanian       1.00      0.95      0.97       102\n",
      "     Russian       0.98      1.00      0.99       110\n",
      "     Spanish       0.95      0.95      0.95       106\n",
      "     Swedish       0.97      0.99      0.98        99\n",
      "       Tamil       1.00      0.98      0.99       109\n",
      "        Thai       1.00      0.97      0.99       108\n",
      "     Turkish       0.99      0.98      0.98        87\n",
      "        Urdu       1.00      0.90      0.95       112\n",
      "\n",
      "    accuracy                           0.95      2200\n",
      "   macro avg       0.96      0.95      0.96      2200\n",
      "weighted avg       0.96      0.95      0.96      2200\n",
      "\n",
      "Figure(1000x800)\n",
      "2025-02-20 08:59:26,716 - langlens.langlens.evaluation - INFO - Variance explained by PCA: [0.3236703  0.13664327]\n",
      "Figure(800x600)\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "We observe a validation set accuracy and a F1 score of 0.96, this is not a significantly different result (if at all, worse) compared to the previous one. We refrain from performing range compression as it requires a lot of hand-crafted rules, described in [3].",
   "id": "ddbf57a37a46ed07"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-20T13:17:46.375735Z",
     "start_time": "2025-02-20T13:17:46.365849Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os\n",
    "\n",
    "# Clean up\n",
    "os.remove(\"../data/wili_subset_khan_cleaned.csv\")"
   ],
   "id": "4c7e83a451d3d6e1",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# References\n",
    "1. T. Jauhiainen, M. Lui, M. Zampieri, T. Baldwin, and K. Lindén, “Automatic Language Identification in Texts: A Survey,” 2019.\n",
    "2. C.-O. Truică, J. Velcin, and A. Boicea, “Automatic Language Identification for Romance Languages using Stop Words and Diacritics,” Jun. 2018, doi: 10.1109/SYNASC.2015.45.\n",
    "3. A. Simões, J. J. Almeida, and S. D. Byers, “Language identification: A neural network approach,” in OpenAccess Series in Informatics, Schloss Dagstuhl- Leibniz-Zentrum fur Informatik GmbH, Dagstuhl Publishing, 2014, pp. 251–265. doi: 10.4230/OASIcs.SLATE.2014.251.\n",
    "\n",
    "\n"
   ],
   "id": "8de9a2a7c872f6f0"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
