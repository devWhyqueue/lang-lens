\begin{abstract}
\noindent Language identification (LID) is a fundamental problem in natural language processing (NLP) that involves determining the language of a given text. In this work, we frame LID as a multiclass classification task and evaluate classical machine learning approaches, including na√Øve Bayes, support vector machines (SVM), and logistic regression. Using a preprocessed subset of the WiLI-2018 dataset, we conduct hyperparameter selection over $n$-gram representation (character vs. word), vocabulary size, and additional noise removal. We report vocabulary coverage, accuracy, and macro F1-score as evaluation metrics. Our results indicate that SVM with character unigrams, a vocabulary size of 500, and no additional preprocessing achieves the highest accuracy and F1-score of 0.97. All models demonstrate strong overall performance, suggesting that traditional ML methods remain effective for LID. However, closely related languages, such as those in the Romance family, pose challenges, highlighting potential areas for improvement through deep learning techniques. The full codebase and experimental notebooks are publicly available on GitHub.\footnote{\url{https://github.com/devWhyqueue/lang-lens}}
\end{abstract}